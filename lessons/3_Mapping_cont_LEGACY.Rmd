---
title: "Mapping_2"
output: html_document
date: "2024-01-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning Objectives

Welcome to R Geospatial Fundamentals. Our goal for today's workshop is:

1.  **Understand how different classification schemes can be used to emphasize key aspects of geospatial data**

------------------------------------------------------------------------

Throughout this workshop series, we will use the following icons:

üîî **Question**: A quick question to help you understand what's going on.

ü•ä **Challenge**: Interactive exercise. We'll go through these in the workshop!

‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.

üí° **Tip**: How to do something a bit more efficiently or effectively.

üìù **Poll**: A zoom poll to help you learn.

üé¨ **Demo**: Showing off something more advanced so you know what you can use R for in the future

------------------------------------------------------------------------

# Classification schemes

Another way to make more meaningful maps is to improve the way in which data values are associated with colors.

The common alternative to the proportional color maps we've created thus far is to use a **classification scheme** to create a **graduated color map**.

A **classification scheme** is a method for binning continuous data values into multiple classes (often 4-7) and then associate those classes with the different colors in a color palette.

The commonly used classification schemes include equal interval, quantiles, natural breaks, heads/tails, and manual schemes. The `?tm_polygons` documentation, under the `style` argument provides keywords names for the different classification styles.

Let's explore each classifications schemes:

------------------------------------------------------------------------

[Load libraries]{.underline}

```{r}

library(here) #provides a here() command that builds file paths from the project directory
library(sf)
library(tmap)
```

[Load in datasets]{.underline}

```{r}

#read in the shapefile
counties = st_read(here("data",
                              "california_counties", 
                              "CaliforniaCounties.shp"))

#read in the schools_sf shapefile using a slightly different syntax
schools_sf = st_read(here("data",
                               "California_Schools_2019-20",
                               "SchoolSites1920.shp"))


#data sources: 
# counties: https://gis.data.ca.gov/datasets/8713ced9b78a4abb97dc130a691a8695 
# schools_sf: https://gis.data.ca.gov/datasets/f7f818b0aa7a415192eaf66f192bc9cc
```

**A. Classifying data based on Equal intervals** (e.g. **pretty)**

An Equal-interval classification segments data into equal-size data ranges (e.g., values within 0-10, 10-20, 20-30, etc.)

-   pros:
    -   best for data spread across the entire range of values
    -   easily understood by map readers
-   cons:
    -   avoid if you have highly skewed data or a few big outliers because one or more of the bins may have no data observations

üîî **Question**: Do you recall what transforming count data into density data means?

```{r}

#create a new column that contains the multi_race density data
counties$POP12_SQMI<-counties$POP2012/counties$SQ_MI
```

[plot data using the equal interval classification scheme]{.underline}

```{r}
tmap_mode('plot')
# Plot population density - mile^2
pretty_clasi_plot<- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              alpha = 0.9,
              style = "pretty", #style of the break, pretty = equal interval
              title = "Population Density per mi^2 Equal-Interval Scheme")

pretty_clasi_plot
```

In instances where proportions don't make the data more representative, we can look to other classification schemes.

**B. Classifying data based on Quantiles**

A quantile scheme distributes an equal number of observations in each bin

-   pros:

    -   looks nice, because it best spreads colors across full set of data values
    -   thus, it's often the default scheme for mapping software

-   cons:

    -   the bin ranges are based on the number of observations, not on the data values
    -   thus, different classes can contain very similar or very different data values

We'll now plot the 'POP12_SQMI' counties data using the quantile style

üîî **Question**: What should you change `"pretty"` to?

-   Hint: you can take a look at the `?tm_polygons` documentation to find the correct syntax

[plot data using the quantile classification scheme]{.underline}

```{r}

tmap_mode('plot')
# Plot population density - mile^2
quant_clasi_plot<- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              alpha = 0.9,
              style = "quantile", #style of the break
              title = "Population Density per mi^2 Quantile Scheme") 
quant_clasi_plot
```

**C. Classifying data based on Natural breaks**

Natural breaks minimizes within-class variance and maximize between-class differences. (Don't worry too much about the nuances of each break, the goal is mostly to show you that you have many options).

-   pros:
    -   great for exploratory data analysis, because it can identify natural groupings
-   cons:
    -   class breaks are best fit to one dataset, so the same bins can't always be used for multiple years

[plot data using one type of natural break (fisher)]{.underline}

```{r}
tmap_mode('plot')
# Plot population density - mile^2
nat_clasi_plot <- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              style = "fisher", #style of the break
              alpha = 0.9,
              title = "Population Density per mi^2")
nat_clasi_plot
```

Note the range of each bin.

**D. Classifying data Manually**

user-defined classification schemes allow the user to manually set the breaks for the bins using the `breaks()` argument.

-   pros:
    -   especially useful if you want to slightly change the breaks produced by another scheme
    -   can be used as a fixed set of breaks to compare data over time
-   cons:
    -   more work involved because breaks are made manually

[plot data using a manual classification scheme]{.underline}

```{r}

man_clasi_plot <- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              style = 'fixed',
              breaks = c(0, 50, 100, 200, 300, 400, max(counties$POP12_SQMI)),
              #labels = c('<50','50 to 100','100 to 200','200 to 300','300 to 400','>400'),
              title = "Population Density per Square Mile Manual Scheme")
man_clasi_plot
```

------------------------------------------------------------------------

## ü•ä Challenge 3: Classifying data based on **Head/Tails**

**D.** Classifying data based on **Head/Tails**

The heads/Tails scheme is tailored to data with a heavy-tailed distributions

1.  search the `?tm_polygons` documentation to find the appropriate argument for a heads/tails classification

2.  create a plot named `HeadsTails_clasi_plot` using a heads/tails scheme

3.  create a combined variable named `combined_clasi_plots` that shows the 4 plots we've now created in pats A-D.

```{r}

# YOUR CODE HERE
```

See the documentation `?classIntervals` or sources such as [Geocomputation with R](https://geocompr.robinlovelace.net/adv-map.html) ebook for more information on data classifications.

Aso note that there are other mapping packages including

-   `mapview`: for a quick and easy interactive map

-   `leaflet`: for highly custom interactive maps that you can output and host on a website

-   `shiny`: for interactive R based applications that use leaflet maps

------------------------------------------------------------------------

## Key Points

In this section of the workshop, we looked at how

-   Changing the way in which the different shades and colors of a thematic map are associated with data, via binning the data using different classification schemes, enhances the type of information that can be conveyed.

Now that we know some useful ways to visualize data, in the next segment of the workshop, we will focus on spatial analyses ...
